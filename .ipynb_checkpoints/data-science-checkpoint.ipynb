{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "\n",
    "One of the most important aspects of designing any machine learning system is to __understand your data__. In this notebook, we're going to load and visualize some very basic datasets so that you understand some of the various tools available to you in Python for working with data. You should __always try to visualize your data__ before you use it in any type of algorithm. First we will look at a random dataset, and then we'll use `sklearn` to study the Iris dataset and other famous datasets.\n",
    "\n",
    "_Note: some code segments have TODO comments in them. These comments are optional exercises for you to modify the code in a useful way, however they are not meant to be restrictive. Feel free to modify the code in this notebook any way you like; it's a great way to practice your coding skills._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "You should have your own Anaconda virtual environment with all of the necessary Python modules installed. You can check by trying to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage.io\n",
    "import sklearn\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating Random Data\n",
    "\n",
    "Let's begin by generating a random dataset of 2-D data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set a random seed to get \"reproducible\" randomness\n",
    "np.random.seed(912)\n",
    "\n",
    "# define a function to generate random data\n",
    "def random_data(n_samples):\n",
    "    \"\"\"\n",
    "    Generates random data in two dimensions.\n",
    "    Returns a (n_samples, 2) numpy array.\n",
    "    \"\"\"\n",
    "    # define the parameters (mean, covariance) of a normal distribution\n",
    "    mean = [0.5, 0.5]\n",
    "    cov = [[1,0], [0,100]]\n",
    "\n",
    "    # generate n_samples of 2D data\n",
    "    X = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this data looks like. We'll start small and draw 50 samples from our function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random_data(50)\n",
    "print('Dataset shape: (%d, %d)' % X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we have some data to play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Univariate Data\n",
    "\n",
    "Let's try to generate some more samples and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 random samples\n",
    "X = random_data(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic aspects of a dataset is its __dimensionality__. For example, if a dataset is 2-dimensional, it means that the dataset has two variables that are influencing the outcomes. These dimensions are also called the __features__ of a dataset. In this case, `X` is our dataset, and it has two features (we'll call them \"X_1\" and \"X_2\"). Let's plot each of these features individually using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate indices for X\n",
    "idx = range(len(X))\n",
    "\n",
    "# initialize a 2x1 figure for plotting\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "\n",
    "# plot X_1 on the left\n",
    "ax1.scatter(idx, X[:, 0], c='b')\n",
    "ax1.set_title('X_1')\n",
    "ax1.set_xlabel('Index')\n",
    "ax1.set_ylabel('Value')\n",
    "\n",
    "# plot X_2 on the right\n",
    "ax2.scatter(idx, X[:, 1], c='r')\n",
    "ax2.set_title('X_2')\n",
    "ax2.set_xlabel('Index')\n",
    "ax2.set_ylabel('Value')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've simply plotted all of the samples in a line, separately for each feature. This visualization is fairly straightforward, but it's not how we typically look at data of this type. That is, since this dataset consists of many independent samples, a better way to visualize this data is to use a __histogram__. The `matplotlib` library has a histogram function called `plt.hist()`, but here we're going to use another library called `seaborn`, which provides some fancier plotting functions on top of `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a 2x1 figure for plotting\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharex=True)\n",
    "\n",
    "# plot histogram of X_1 on the left\n",
    "sns.histplot(X[:, 0], kde=True, color='b', ax=ax1)\n",
    "ax1.set_title('X_1')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# plot histogram of X_2 on the right\n",
    "sns.histplot(X[:, 1], kde=True, color='r', ax=ax2)\n",
    "ax2.set_title('X_2')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see more clearly what the differences are between X_1 and X_2. Both features are centered at 0 (to be exact, they are centered at 0.5). However, X_1 spreads out _very little_ while X_2 spreads out _a lot_. We typically use these two properties to broadly describe data: the center of the data is called the __mean__ and the spread is called the __variance__.\n",
    "\n",
    "Furthermore, both X_1 and X_2 look like \"bell curves\". As it turns out, each of these features were generated using a __Gaussian distribution__ (__normal distribution__):\n",
    "\n",
    "$$f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}$$\n",
    "\n",
    "The Gaussian distribution is used in many fields because it occurs so frequently in nature, and because any Gaussian distribution can be described by a mean and a variance. For example, we can describe how X_1 and X_2 were generated:\n",
    "\n",
    "$$x_1 \\sim \\mathcal{N}(0.5, 1)$$\n",
    "$$x_2 \\sim \\mathcal{N}(0.5, 100)$$\n",
    "\n",
    "Now for a software engineering exercise: let's combine the data generation and visualization into a single function so that we can repeat and experiment with this process easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate and plot a random dataset\n",
    "def plot_random_data(n_samples, colors=['b', 'r']):\n",
    "    \"\"\"\n",
    "    Generates n_samples random data from a distribution and \n",
    "    plots each dimension in a separate plot.\n",
    "    \n",
    "    Args:\n",
    "    n_samples: Number of samples to be generated. \n",
    "    colors (optional): The colors for the different plots.\n",
    "    \"\"\"\n",
    "    # generate random dataset\n",
    "    mean = [0.5, 0.5]\n",
    "    cov = [[1,0], [0,100]]\n",
    "\n",
    "    X = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "\n",
    "    # initialize a 2x1 figure for plotting\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharex=True)\n",
    "\n",
    "    # plot histogram of X_1 on the left\n",
    "    sns.histplot(X[:, 0], kde=True, color=colors[0], ax=ax1)\n",
    "    ax1.set_title('X_1')\n",
    "    ax1.set_xlabel('Value')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "\n",
    "    # plot histogram of X_2 on the right\n",
    "    sns.histplot(X[:, 1], kde=True, color=colors[1], ax=ax2)\n",
    "    ax2.set_title('X_2')\n",
    "    ax2.set_xlabel('Value')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # TODO: modify this function so that you can specify the mean and variance of each dimension!\n",
    "    # TODO: modify this function so that you can specify the number of dimensions to use!\n",
    "\n",
    "# test the function on several dataset sizes\n",
    "N_values = [10, 100, 1000, 10000]\n",
    "\n",
    "for N in N_values:\n",
    "    print('%d samples:' % (N))\n",
    "    plot_random_data(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the histograms look more like bell curves as the sample size increases. In fact, most machine learning algorithms perform better when there is a sufficiently large amount of data. Having plenty of clean data helps prevent the algorithm from being influenced by outliers and noise in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Multivariate Data\n",
    "\n",
    "So far we've visualized _univariate_ (1-dimensional) data, but now let's visualize the entire dataset. We're going to use another function from `seaborn` called `jointplot` to visualize _bivariate_ (2-dimensional) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 2-D dataset\n",
    "X = random_data(10000)\n",
    "\n",
    "# create a pandas dataframe with the data and column names\n",
    "df = pd.DataFrame(X, columns=['X_1', 'X_2'])\n",
    "\n",
    "# show the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dataset with seaborn\n",
    "sns.jointplot(x='X_1', y='X_2', data=df, kind='scatter')\n",
    "plt.show()\n",
    "\n",
    "# TODO: try other options for kind ('kde', 'hex', etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks cool! We essentially just collapsed the two plots from before into one. The `jointplot` function actually plots three things: the scatter plot, and two histograms for each of the individual features. We also introduced something called a `DataFrame`, a class from the `pandas` library which allows us to package our data with row/column names into a single object. This way we can name the features in our dataset, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iris Dataset\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "Now that we've seen how to generate and visualize random data, let's move on to the famous Iris dataset. Widely considered to be the \"Hello World\" for datasets, the Iris dataset was created in the early 1900's by the British statistician and biologist Ronald Fisher. This dataset has four features, which correspond to different measurements of an Iris flower, and 150 samples, which correspond to individual flowers. The dataset also has __labels__ or __targets__ associated with the data; that is, each sample is labeled according to its species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# show dataset shape\n",
    "print('Iris data: (%d, %d)' % iris.data.shape)\n",
    "print('Iris targets: (%d,)' % iris.target.shape)\n",
    "\n",
    "# show feature names\n",
    "print('Feature names: ', iris.feature_names)\n",
    "\n",
    "# show label names\n",
    "print('Label names: ', iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the __shape__ of the data denotes the number of samples and features, respectively, in the data. We can think of this dataset as an Excel spreadsheet with 150 rows and 4 columns of values, as well as an extra column for the labels. Even the smallest machine learning systems typically use datasets with thousands of samples (if not more) and any number of features depending on the application. Using the tools we've developed so far, we can at least visualize any 2 dimensions of a dataset.\n",
    "\n",
    "Now, let's load just a slice of the dataset and see what it looks like. As before we'll write code for a specific example and then we'll put it into a function so that we can repeat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# plot the first two features\n",
    "x = iris.data[:, 0]\n",
    "y = iris.data[:, 1]\n",
    "\n",
    "plt.scatter(x, y, c=iris.target)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.show()\n",
    "\n",
    "# TODO: plot the 1st and 3rd features instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cool feature of `plt.scatter()` is that instead of using a single color (`c='b'`) you can provide a list of labels (`c=iris.target`) and matplotlib will automatically color each data point according to its label. In this case, matplotlib picked three colors for the three species in the Iris dataset.\n",
    "\n",
    "As you can see, the distribution of this data is much more... interesting... than that of the random data from before. In reality, most real-world data is much more complex, which makes plots like these all the more important for understanding our data and using the right machine learning algorithms. This plot, for example, shows us that there seems to be two clusters of data above and below, and that we could probably draw a straight line to separate the two clusters. A dataset with this property is called _linearly separable_ since it can be cleanly separated into groups by one or more straight lines.\n",
    "\n",
    "However, adding color shows us that while one of the species is separated pretty well, the other two species are quite mangled together. What does this mean for us when designing a machine learning system for this data? It means that for whatever task we try to do with this data, such as classification or clustering, any machine learning algorithm we use will probably have more difficulty telling these two species apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Dataframes\n",
    "\n",
    "Let's take a brief detour to go a bit deeper with `pandas` dataframes. Like we said before, a dataframe is essentially a Numpy array with row names and column names, which we can use to package data, labels, sample names (if applicable), and feature names into a single object. We can manipulate dataframes in many powerful ways using Python's array indexing syntax, and there are a couple of minute differences between dataframes and Numpy arrays.\n",
    "\n",
    "First, let's create a dataframe for the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# initialize Numpy arrays for data and labels\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# initialize dataframe\n",
    "df = pd.DataFrame(data=np.c_[X, y], columns=np.append(iris.feature_names, ['target']))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between a Numpy array and a pandas Dataframe is the use of the indexing brackets (`[]`). With a Numpy array, the index is a row number and returns a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But with a pandas Dataframe, the index is actually a column name and returns a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sepal length (cm)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can still use a dataframe like a Numpy array if you need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, :].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the real power in a dataframe is the ability to select columns and filter by columns. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the versicolor samples\n",
    "print(iris.target_names)\n",
    "\n",
    "# 'versicolor' is second in the list, so the numerical value is 1\n",
    "df[df['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first column of the versicolor samples\n",
    "df[df['target'] == 1]['sepal length (cm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select multiple columns\n",
    "df[['sepal length (cm)', 'sepal width (cm)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Distributions\n",
    "\n",
    "Building off of the previous section where we selected just the Iris samples that belong to a particular class, another basic visualization that we can do is to separate the data by class to see how the distributions vary for each class. In other words, we will visualize the __conditional distribution__ for each class in the Iris dataset. To do this we'll use the `FacetGrid` class in `seaborn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "iris.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset (from seaborn)\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# plot a FacetGrid for each feature\n",
    "features = iris.columns[:-1]\n",
    "\n",
    "for feature in features:\n",
    "    g = sns.FacetGrid(iris, col='species', margin_titles=True)\n",
    "    g.map(plt.hist, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Scatter Plots\n",
    "\n",
    "Now, let's write a function that will allow us to plot any two features against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset (from sklearn again)\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "def plot_iris_2d(iris, columns=[0, 1]):\n",
    "    \"\"\"\n",
    "    Plots the Iris dataset with given list of column indices.\n",
    "    Only the first two column indices are used.\n",
    "    \n",
    "    Args:\n",
    "    iris: Iris dataset loaded from sklearn\n",
    "    columns: The column indices. By default it'll plot the first and second features.\n",
    "    \"\"\"\n",
    "\n",
    "    # extract x and y axes\n",
    "    x, y = iris.data[:, columns[0]], iris.data[:, columns[1]]\n",
    "\n",
    "    # plot x and y\n",
    "    plt.scatter(x, y, c=iris.target)\n",
    "    plt.xlabel(iris.feature_names[columns[0]])\n",
    "    plt.ylabel(iris.feature_names[columns[1]])\n",
    "    plt.show()\n",
    "\n",
    "    # TODO: add a legend to the scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_2d(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_2d(iris, [0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_2d(iris, [0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris_2d(iris, [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are a lot of \"angles\" from which we can view the Iris dataset, but no single plot provides a complete view. However, `seaborn` has another supercharged plotting feature that is perfect for this kind of dataset, called `PairGrid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris dataset (from seaborn)\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# plot PairGrid showing 1-D histograms and 2-D scatter plots\n",
    "g = sns.PairGrid(iris, hue='species')\n",
    "g = g.map_diag(plt.hist)\n",
    "g = g.map_offdiag(plt.scatter)\n",
    "g = g.add_legend()\n",
    "\n",
    "# TODO: change the lower half of the PairGrid to KDE plots (see sns.kdeplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `PairGrid` is _really_ powerful because it essentially allows us to see a lot of angles all at once. It's also very configurable; we can \"map\" the diagonal, upper, and lower halves each to a separate plotting function, and in this case we can color the data by label and add a legend with little effort. The only downside is that `PairGrid` does not scale well to large datasets; as you increase the number of features (the Iris dataset only has four), the number of plots increases rapidly. You can still use `PairGrid` with a large dataset by selecting only a few features from the dataset.\n",
    "\n",
    "To learn more about what all seaborn can do, check out the [examples gallery](http://seaborn.pydata.org/examples/) and the API docs on the seaborn website.\n",
    "\n",
    "Let's look again at this issue of separability. It's clear that these two species (which we can identify as `versicolor` and `virginica` thanks to the legend) are stuck together, and any machine learning algorithm we throw at the data will have a hard time distinguishing these two. How can we deal with this problem? The primary way to make data easier to separate is to __add more features__; in other words, add a third or fourth axis along which we might be able to separate the two classes. Now we can't go back to 1936 and tell Ronald Fisher to add more types of measurements, but remember that __we're only looking at two dimensions at a time__. For any given scatter plot above there are two dimensions being excluded, which means that `versicolor` and `virginica` may in fact be separable if we just use the entire dataset instead of only two dimensions. We will revisit this question later when we begin to look at machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contingency Tables\n",
    "\n",
    "The four features in the Iris dataset are examples of __numerical__ variables, because they consist of continuous values. A scatter plot is a good way to compare two numerical variables, to see if they are related to each other. But the species label for Iris is a __categorical__ variable, because it consists of a few discrete values, or categories. So what if we want to compare two categorical variables?\n",
    "\n",
    "To answer this, let's load a dataset that has more categorical data for us to use. How about the Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# select two categorical variables\n",
    "x = titanic['class']\n",
    "y = titanic['survived']\n",
    "\n",
    "# convert categorical data to integers\n",
    "x_values = sorted(list(set(x)))\n",
    "y_values = sorted(list(set(y)))\n",
    "x = [x_values.index(x_i) for x_i in x]\n",
    "y = [y_values.index(y_i) for y_i in y]\n",
    "\n",
    "# create contingency table\n",
    "ct = pd.DataFrame(np.zeros((len(y_values), len(x_values))), index=y_values, columns=x_values, dtype=np.int32)\n",
    "\n",
    "for x_i, y_i in zip(x, y):\n",
    "    ct.iloc[y_i, x_i] += 1\n",
    "\n",
    "sns.heatmap(ct, annot=True, fmt='d', cbar=False, square=True)\n",
    "plt.xlabel('class')\n",
    "plt.ylabel('survived')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the relationship between class and survival rate of the Titanic passengers. Not very surprising.\n",
    "\n",
    "So now you have a toolbox of different visualizations based on the number and type of variables that you want to look at:\n",
    "- __one numerical variable__: histogram\n",
    "- __one categorical variable__: bar plot\n",
    "- __two numerical variables__: scatter plot\n",
    "- __two categorical variables__: contingency table\n",
    "- __one numerical, one categorical variable__: bar plot, facet grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from the Filesystem\n",
    "\n",
    "It's easy to load data on-demand from scikit-learn or seaborn, but eventually you'll need to know how to load real data from the filesystem. Here we'll show you quickly how to load two datasets: a CSV file and an image dataset.\n",
    "\n",
    "### Structured Data\n",
    "\n",
    "The Iris dataset is a good example of __structured data__, which is essentially any type of data that can be represented easily as a table. Structured data can be stored in an Excel spreadsheet, or a relational database, or a __comma-separated values (CSV)__ file. A CSV file is like a spreadsheet -- each row is on a line, and each column is separated by a comma. In fact, you can load CSV files directly into Excel and view them. In Python, we will use the `read_csv` function in pandas to load CSV files. Here's an example where we load the Iris dataset directly from the filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the CSV file from the Internet\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your CSV file doesn't provide column names, you can provide them yourself\n",
    "names = [\n",
    "    'sepal length (cm)',\n",
    "    'sepal width (cm)',\n",
    "    'petal length (cm)',\n",
    "    'petal width (cm)',\n",
    "    'target'\n",
    "]\n",
    "\n",
    "# we'll save them to a file and then load them back so you can see what it looks like\n",
    "np.savetxt('iris.names', names, fmt='%s')\n",
    "\n",
    "names = np.loadtxt('iris.names', dtype=str, delimiter=',')\n",
    "\n",
    "iris = pd.read_csv('iris.data', header=None, names=names)\n",
    "\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the dataset into data and labels\n",
    "X = iris.iloc[:, 0:4]\n",
    "y = iris['target']\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured Data\n",
    "\n",
    "Now that you've seen an example of structured data, __unstructured data__ is essentially everything else -- images, video, audio, 3D objects, etc -- if it can't fit in a table, it's probably unstructured data. So for this example let's focus on image data\n",
    "\n",
    "Image data is a bit more complicated to manage. Whereas structured data is typically stored in a single CSV file, even a large file, we typically store images in individual files so that we can view them easily. We can still load an image dataset into a single numpy array, but we'll need to interact with the file system a bit more. In this example we'll use the [Digits dataset](https://scikit-learn.org/stable/datasets/index.html#digits-dataset) from `sklearn`. Since the dataset comes pre-packaged in a numpy array, we'll save the data as individual images and then load it back, so that you see both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the digits dataset from sklearn\n",
    "digits = sklearn.datasets.load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "classes = digits.target_names\n",
    "\n",
    "# convert data to uint8\n",
    "X = X.astype(np.uint8)\n",
    "\n",
    "# reshape data to be 8x8\n",
    "X = X.reshape(X.shape[0], 8, 8)\n",
    "\n",
    "# print information about dataset\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "print('classes:', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove previous directory tree\n",
    "!rm -rf digits\n",
    "\n",
    "# create directory tree for digits dataset\n",
    "os.mkdir('digits')\n",
    "\n",
    "for c in classes:\n",
    "    os.mkdir('digits/%s' % c)\n",
    "\n",
    "# save each image to a pgm file in class sub-directory\n",
    "for i in range(len(X)):\n",
    "    fname = 'digits/%s/%04d.pgm' % (classes[y[i]], i)\n",
    "    skimage.io.imsave(fname, X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have the digits dataset in a directory called \"digits\" with the following directory structure:\n",
    "```\n",
    "digits/\n",
    "    0/\n",
    "        0000.pgm\n",
    "        ...\n",
    "    1/\n",
    "    2/\n",
    "    ...\n",
    "    9/\n",
    "```\n",
    "\n",
    "List the directory to see for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lR digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count the total number of images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls digits/**/* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizing images by class is one of the most common ways to store an image dataset. That way you can infer the label for each image directory from the it's parent directory. Note however that your image dataset may have multiple labels (for example, face images labelled by person and other attributes such as age, gender, emotion, etc), so you must make sure to group images by the label type that you are using.\n",
    "\n",
    "Now that we have a good example dataset in the filesystem, let's load it back in as if it were a new dataset. The code is actually quite similar to what we just did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer class names from the sub-directory names\n",
    "classes = os.listdir('digits')\n",
    "\n",
    "# initialize empty data array and label array\n",
    "n_samples = 1797\n",
    "X = np.empty((n_samples, 8, 8), dtype=np.uint8)\n",
    "y = np.empty((n_samples,), dtype=np.int64)\n",
    "\n",
    "# iterate through sub-directories\n",
    "i = 0\n",
    "\n",
    "for k, class_name in enumerate(classes):\n",
    "    # get list of images in class k\n",
    "    filenames = os.listdir('digits/%s' % class_name)\n",
    "    filenames = ['digits/%s/%s' % (class_name, f) for f in filenames]\n",
    "    \n",
    "    # load each image into numpy array\n",
    "    for fname in filenames:\n",
    "        X[i] = skimage.io.imread(fname)\n",
    "        y[i] = k\n",
    "        i += 1\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# TODO: can you write a function to load an arbitrary image dataset? some considerations\n",
    "# - it should work for any directory name\n",
    "# - it should determine n_samples and the image size automatically (or be given them as parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the data that we loaded to the original data from `sklearn`, it won't match exactly because our data is ordered by class. So it's the same data but ordered differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.mean() - digits.data.mean())\n",
    "print(y.mean() - digits.target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Understand Your Data\n",
    "\n",
    "The Iris dataset is one of the simplest datasets around, so it's really easy to understand from just a few visualizations. To practice your data-wrangling skills, pick one of the toy datasets from [scikit-learn](http://scikit-learn.org/stable/datasets/index.html#toy-datasets) or [seaborn](http://seaborn.pydata.org/generated/seaborn.load_dataset.html#seaborn.load_dataset) and create some visualizations that help you understand the data. Some good visualizations to get started include:\n",
    "- heatmaps\n",
    "- histograms / violinplots / kde plots\n",
    "- bar plots\n",
    "- pairwise scatter plots\n",
    "- regression plots\n",
    "\n",
    "`seaborn` provides all of these methods and more, and the [examples gallery](http://seaborn.pydata.org/examples/) is a great way to learn about them. Keep in mind that each type of visualization can be used in multiple ways, and some visualizations are more useful than others for a particular dataset.\n",
    "\n",
    "As you examine your dataset of choice, here are some basic questions to ask yourself:\n",
    "- How many __samples__ are in the dataset? How many __features__? How many __labels__?\n",
    "- Can you tell the difference between the features and the labels?\n",
    "- What is the __mean__ and __variance__ of each feature? More broadly, which features are \"spread out\" more?\n",
    "- Are any of the features __correlated__ with each other, or otherwise related in some interesting way?\n",
    "- Is the data __separable__? That is, is it easy to visually separate the data by labels?\n",
    "\n",
    "If you can visualize and understand one of these toy datasets, great! Try some of the other datasets! Every dataset is unique and will probably teach you something new about how to understand data through visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load and visualize another toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue: Brave New World\n",
    "\n",
    "This notebook should give you the basic skills you need to be able to load and visualize datasets. Once you've acquired some experience with one of the toy datasets, it's time to step out into the real world. Check out the [Datasets](https://cufctl.github.io/creative-inquiry/skills/datasets.html) page on the creative inquiry website for links to various dataset repositories, as well as instructions for downloading datasets from Kaggle.\n",
    "\n",
    "As you browse these repositories, think about the kinds of datasets that would interest you. Don't limit yourself to school-related topics -- you can find data on just about anything! Look for datasets related to your hobbies, your passions, or a question that has always bothered you... there might be a dataset out there just for you. Make it fun! Find a dataset that you will enjoy working with. Whatever you settle on over the next few weeks will be the basis for your semester project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
